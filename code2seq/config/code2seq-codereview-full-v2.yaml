data_folder: ./data/code2seq/combined_full

checkpoint: null

seed: 7
# Training in notebooks (e.g. Google Colab) may crash with too small value
progress_bar_refresh_rate: 1
print_config: true

wandb:
  project: Code2Seq -- java-small
  group: null
  offline: false

data:
  url: https://s3.eu-west-1.amazonaws.com/datasets.ml.labs.aws.intellij.net/java-paths-methods/java-small.tar.gz
  num_workers: 0

  # Each token appears at least 10 times (98.5% coverage)w
  labels_count: 10
  max_label_parts: 100 #ターゲットの最大シンボル数
  # Each token appears at least 1000 times (99.2% coverage)
  tokens_count: 1000 
  max_token_parts: 15 #　読み込まれるサブトークンの最大数
  path_length: 15 #context　pathのlength

  max_context: 200 #
  random_context: false

  batch_size: 512 #　バッチサイズ
  test_batch_size: 512 #　テスト用のバッチサイズ

model:
  # Encoder
  embedding_size: 128
  encoder_dropout: 0.25
  encoder_rnn_size: 128
  use_bi_rnn: true
  rnn_num_layers: 1

  # Decoder
  decoder_size: 160
  decoder_num_layers: 1
  rnn_dropout: 0.5

optimizer:
  optimizer: "Momentum"
  nesterov: true
  lr: 0.01
  weight_decay: 0
  decay_gamma: 0.95

train:
  n_epochs: 60 # エポック数
  patience: 15 # eary stopping時の最小学習数
  clip_norm: 5
  teacher_forcing: 1.0
  val_every_epoch: 1
  save_every_epoch: 1 #モデル保存して検証
  log_every_n_steps: 10
